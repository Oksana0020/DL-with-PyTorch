{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZhfEZwXv4Pt48Tv6KMDJe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oksana0020/DL-with-PyTorch/blob/main/Fully_connected_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding fully connected layers and model summary"
      ],
      "metadata": {
        "id": "0v3qGYTfnm4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setting Up PyTorch Environment"
      ],
      "metadata": {
        "id": "ZpYg1cRq9C1w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VwzlO-gqnmGi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Defining Fully Connected Layers in PyTorch"
      ],
      "metadata": {
        "id": "YQGHRM-49FRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNWithFC(nn.Module):\n",
        "    def __init__(self, input_channels=1, image_height=28, image_width=28, num_classes=10):\n",
        "        super(CNNWithFC, self).__init__()\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.image_height = image_height\n",
        "        self.image_width = image_width\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # --------------------------\n",
        "        # Convolutional layers\n",
        "        # Conv2d(in_channels, out_channels, kernel_size, padding)\n",
        "        # With kernel_size=3 and padding=1, spatial dims (H, W) are preserved.\n",
        "        # Output shape after conv1: (batch_size, 32, image_height, image_width)\n",
        "        # --------------------------\n",
        "        self.conv1 = nn.Conv2d(self.input_channels, 32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Output shape after pool1: (batch_size, 32, image_height/2, image_width/2)\n",
        "        # After conv2 (padding=1): (batch_size, 64, image_height/2, image_width/2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # --------------------------\n",
        "        # Max pooling layer\n",
        "        # Each pooling operation halves the spatial dimensions\n",
        "        # --------------------------\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # --------------------------\n",
        "        # Calculate flattened size after conv + pool layers\n",
        "        # After conv1 (dims preserved) then pool1 (dims halved):\n",
        "        # height_after_pool1 = image_height // 2\n",
        "        # width_after_pool1  = image_width  // 2\n",
        "        #\n",
        "        # After conv2 (dims preserved) then pool2 (dims halved again):\n",
        "        pooled_height = self.image_height // 2 // 2\n",
        "        pooled_width  = self.image_width  // 2 // 2\n",
        "\n",
        "        # The number of output channels from conv2 is 64\n",
        "        self.fc1_in_features = 64 * pooled_height * pooled_width\n",
        "\n",
        "        # --------------------------\n",
        "        # Fully connected layers\n",
        "        # --------------------------\n",
        "        self.fc1 = nn.Linear(self.fc1_in_features, 128)\n",
        "        self.fc2 = nn.Linear(128, self.num_classes)  # output layer (logits)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)       # ReLU activation\n",
        "        x = self.pool(x)    # Max pooling\n",
        "\n",
        "        # Convolutional Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)       # ReLU activation\n",
        "        x = self.pool(x)    # Max pooling\n",
        "\n",
        "        # Flatten before FC layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully Connected Block\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)     # raw logits (no softmax for CrossEntropyLoss)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "f0GA7Iw0oSBV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Visualizing the Model Summary"
      ],
      "metadata": {
        "id": "ClGYijgn9Ixy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure torchsummary is installed.\n",
        "# If not installed in your environment, uncomment the next line:\n",
        "# !pip install torchsummary\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "# Define input parameters (MNIST example)\n",
        "input_channels = 1\n",
        "image_height = 28\n",
        "image_width = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Create model instance\n",
        "model = CNNWithFC(\n",
        "    input_channels=input_channels,\n",
        "    image_height=image_height,\n",
        "    image_width=image_width,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "# input_size format: (channels, height, width)\n",
        "summary(model, input_size=(input_channels, image_height, image_width))\n"
      ],
      "metadata": {
        "id": "DzTwACQToWWJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6cd1e57-d794-480b-ea61-11195dd9ddf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "         MaxPool2d-2           [-1, 32, 14, 14]               0\n",
            "            Conv2d-3           [-1, 64, 14, 14]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 7, 7]               0\n",
            "            Linear-5                  [-1, 128]         401,536\n",
            "            Linear-6                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.36\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 1.97\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}