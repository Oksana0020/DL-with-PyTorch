{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXZRtAIwhrNprVB09r7PXw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oksana0020/DL-with-PyTorch/blob/main/LossFunction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions in Deep Learning**\n",
        "\n",
        "In the vibrant world of deep learning, have you ever wondered how machines refine their predictions over time? At the core of this intricate process lies the concept of loss functions—crucial elements steering neural networks toward enhanced accuracy. Consider the task of aiming a dart at a target: each miss informs you how to adjust your next throw. Similarly, loss functions calculate the discrepancy between predictions and actual values, guiding neural networks to optimize their predictions.\n",
        "\n",
        "**Loss function**, interchangeably referred to as a cost or objective function, is a mathematical construct that quantifies the performance of a neural network on given data. It assesses how far the predicted output divergently deviates from the target values, thereby calculating the 'loss.' This loss acts as critical feedback, analogous to a compass, helping the neural network adjust its weights to minimize future errors through the technique of backpropagation.\n",
        "\n",
        "**Why Loss Functions Matter**\n",
        "The significance of loss functions in neural networks cannot be overstated, encompassing several vital roles:\n",
        "\n",
        "Guidance for Learning: Loss functions steer the learning process, pinpointing paths that yield accurate predictions.\n",
        "\n",
        "Model Performance: The chosen loss function affects how the model accounts for various errors, shaping the learning trajectory.\n",
        "\n",
        "Optimization Efficiency: An appropriate loss function enhances learning efficacy—pivotal for training intricate models.\n",
        "\n",
        "Common Loss Functions in Deep Learning\n",
        "Given the diversity of tasks in deep learning, selecting an appropriate loss function tailored to data nature and goals is essential. Here, we'll delve into some prevalent types:\n",
        "\n",
        "Mean Squared Error (MSE)\n",
        "Mean Squared Error is primarily employed in regression problems where predicting continuous values is crucial. It computes the average of squared differences between actual and predicted values, emphasizing larger deviations by squaring them.\n",
        "\n",
        "MSE = (1/N) * Σ (from i=1 to N) (yᵢ - ŷᵢ)²  \n",
        "\n",
        "Use Case: Forecasting stock prices or predicting real estate values.\n",
        "\n",
        "Binary Cross-Entropy\n",
        "Binary Cross-Entropy is ideal for binary classification issues, evaluating the disparity between two probability distributions—actual Bernoulli distribution and predicted distribution.\n",
        "\n",
        "Binary CrossEntropy = (1/N) * Σ (from i = 1 to N) [ yᵢ * log (ŷᵢ) + (1 - yᵢ) * log (1 - ŷᵢ) ]\n",
        "\n",
        "Use Case: Classifying emails as spam or not spam.\n",
        "\n",
        "Categorical Cross-Entropy\n",
        "An extension of binary cross-entropy, categorical cross-entropy caters to multi-class classification contexts, comparing predicted softmax distributions with target distributions.\n",
        "\n",
        "\n",
        "How Loss Functions Work: An Example with PyTorch"
      ],
      "metadata": {
        "id": "V6fYENVOiHCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUqSvHxZiGRZ",
        "outputId": "d59e1340-5bf4-471e-ea10-bde4cb5d6eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.5090978145599365\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a basic linear model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# Generate random inputs and targets\n",
        "inputs = torch.randn(10, 10)\n",
        "targets = torch.randn(10, 1)\n",
        "\n",
        "# Specify a loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Perform a forward pass\n",
        "predictions = model(inputs)\n",
        "\n",
        "# Calculate the loss\n",
        "loss = loss_function(predictions, targets)\n",
        "print('Loss:', loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How optimisers work"
      ],
      "metadata": {
        "id": "eRrPgrdziWbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple linear model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# Generate random inputs and targets\n",
        "inputs = torch.randn(100, 10)\n",
        "targets = torch.randn(100, 1)\n",
        "\n",
        "# Choose an optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Specify a loss function\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "\n",
        "    # Forward pass\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_function(predictions, targets)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHGpzba_iZAY",
        "outputId": "250a270d-f079-4ac2-e18a-c3668f3fae86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.7466\n",
            "Epoch 10, Loss: 1.5067\n",
            "Epoch 20, Loss: 1.3574\n",
            "Epoch 30, Loss: 1.2627\n",
            "Epoch 40, Loss: 1.2014\n",
            "Epoch 50, Loss: 1.1609\n",
            "Epoch 60, Loss: 1.1336\n",
            "Epoch 70, Loss: 1.1148\n",
            "Epoch 80, Loss: 1.1016\n",
            "Epoch 90, Loss: 1.0921\n"
          ]
        }
      ]
    }
  ]
}